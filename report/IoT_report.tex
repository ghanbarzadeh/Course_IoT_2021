% Usefull codes

%\begin{figure}T_
%	\centering 
%	\includegraphics[width=0.5\columnwidth]{swallow.jpg}
%	\caption{Closed-Loop system with PID controller} 
%\label{fig:1} 
%\end{figure}
%
%\begin{enumerate}
%	\item Step input tracking
%	\item Changing reference tracking (Servo)
%	\item Disturbance rejection
%\end{enumerate}
%
%\begin{enumerate}[(\itshape a\normalfont)] % Sub-questions styled as italic letters
%	\item Suppose ``chuck" implies throwing.
%	\item Suppose ``chuck" implies vomiting.
%\end{enumerate}
%
%\begin{center}{\begin{minipage}{0.85\linewidth}
%\begin{lstlisting}[
%	caption={First-degree with time delay approximation},
%	captionpos=t,label={code:1}]
%system = tf(1,[1,9,23,15]);
%[y,t] = step(system);
%
%time_step = t(2)-t(1);
%u = ones(size(y));
%data = iddata(y,u,time_step);
%system_model = procest(data,'P1D');
%\end{lstlisting}
%\end{minipage}}\end{center}
%
%\begin{equation}
%	P(A|B) = \frac{P(B|A)P(A)}{P(B)}
%\label{eq:bayes}
%\end{equation}
%
%	\lstinputlisting[
%		caption=Luftballons Perl Script, % Caption above the listing
%		label=lst:luftballons, % Label for referencing this listing
%		language=Perl, % Use Perl functions/syntax highlighting
%		frame=single, % Frame around the code listing
%		showstringspaces=false, % Don't put marks in string spaces
%		numbers=left, % Line numbers on left
%		numberstyle=\tiny, % Line numbers styling
%	]{luftballons.pl}
%
%\begin{center}
%	\begin{tabular}{l l l}
%		\toprule
%		\textit{Per 50g} & Pork & Soy \\
%		\midrule
%		Energy & 760kJ & 538kJ\\
%		Protein & 7.0g & 9.3g\\
%		Carbohydrate & 0.0g & 4.9g\\
%		Fat & 16.8g & 9.1g\\
%		Sodium & 0.4g & 0.4g\\
%		Fibre & 0.0g & 1.4g\\
%		\bottomrule
%	\end{tabular}
%\end{center}
%
%\begin{table}[hbt]
%\caption{Genetic Algorithm Hyper-Parameters}
%\centering
%\begin{tabular}{l|c}
%\toprule
%Chromosome per generation & $100$ \\
%Encoding & Real \\
%Variables & $K_p,T_i,T_d$ \\
%Lower bounds & $[0, 0.001, 0]$ \\
%Upper bounds & $[200, 5, 0.8]$ \\
%Termination criteria & MaxGen=$300$ \\
%Selection technique & Stochastic uniform \\
%Crossover operation & Arithmatic \\
%Mutation probability & 0.01 \\
%\bottomrule
%\end{tabular}
%\label{tab:1}
%\end{table}
%
%\begin{figure}
%\centering
%\subfloat[Output y]{\includegraphics[width=0.75\columnwidth]{y3.eps}}\quad
%\subfloat[Control signal u]{\includegraphics[width=0.75\columnwidth]{u3.eps}}
%\caption[Simulation results of disturbance rejection]{Simulation results of disturbance rejection}
%\label{fig:1}
%\end{figure}

%----------------------------------------------------------------------------------------

\documentclass[
	12pt, % Default font size, values between 10pt-12pt are allowed
	%letterpaper, % Uncomment for US letter paper size
]{packages/fphw}

\input{packages/packages.tex}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Final Project Report} % Assignment title

\author{Armin Ghanbarzadeh, Yasamin Borhani, Mohammad Hoseinzadeh} % Student name

\date{23 Jan, 2022} % Due date

\institute{K. N. Toosi University of Technology\\ Faculty of Mechanical Engineering - Mechatronics Group } % Institute or school name

\class{IoT} % Course or class name

\instructor{Dr. Najafi} % Professor or teacher in charge of the assignment

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle 
The code can be found at \url{https://github.com/ghanbarzadeh/Course_IoT_2021}.
%----------------------------------------------------------------------------------------
\section*{Part I}

\subsection*{Loading Data}

The dataset contains recorded data from a gearbox working normally and the same gearbox when it has a broken gear tooth. The data is recorded with 4 sensors, each corresponding to the vibration of a point on the gearbox body.
For each class we have a total of 10 excel files. Each of these files has the vibration of the gears at a specific load. We have first merged all the data for each class and in \ref{t1} and \ref{t2}, some statistical properties of the dataset is shown.

\begin{table}[H]
\centering
\begin{tabular}{
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}r 
>{\columncolor[HTML]{FFFFFF}}r 
>{\columncolor[HTML]{FFFFFF}}r 
>{\columncolor[HTML]{FFFFFF}}r 
>{\columncolor[HTML]{FFFFFF}}r 
>{\columncolor[HTML]{FFFFFF}}r }	
\multicolumn{1}{r}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{}}} & {\color[HTML]{212121} \textbf{Sensor 1}} & {\color[HTML]{212121} \textbf{Sensor 2}} & {\color[HTML]{212121} \textbf{Sensor 3}} & {\color[HTML]{212121} \textbf{Sensor 4}} & {\color[HTML]{212121} \textbf{load}} & {\color[HTML]{212121} \textbf{failure}} \\ \cline{2-7} 
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{count}}} & {\color[HTML]{212121} 1.02e+06} & {\color[HTML]{212121} 1.02e+06} & {\color[HTML]{212121} 1.02e+06} & {\color[HTML]{212121} 1.02e+06} & {\color[HTML]{212121} 1.02e+06} & {\color[HTML]{212121} 1.02e+06} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{mean}}} & {\color[HTML]{212121} 3.70e-03} & {\color[HTML]{212121} -1.20e-03} & {\color[HTML]{212121} 8.14e-03} & {\color[HTML]{212121} -1.12e-04} & {\color[HTML]{212121} 4.58e+01} & {\color[HTML]{212121} 0.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{std}}} & {\color[HTML]{212121} 7.38e+00} & {\color[HTML]{212121} 4.43e+00} & {\color[HTML]{212121} 4.11e+00} & {\color[HTML]{212121} 4.52e+00} & {\color[HTML]{212121} 2.83e+01} & {\color[HTML]{212121} 0.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{min}}} & {\color[HTML]{212121} -5.87e+01} & {\color[HTML]{212121} -3.29e+01} & {\color[HTML]{212121} -2.92e+01} & {\color[HTML]{212121} -3.13e+01} & {\color[HTML]{212121} 0.00e+00} & {\color[HTML]{212121} 0.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{25\%}}} & {\color[HTML]{212121} -3.92e+00} & {\color[HTML]{212121} -2.40e+00} & {\color[HTML]{212121} -2.20e+00} & {\color[HTML]{212121} -2.38e+00} & {\color[HTML]{212121} 2.00e+01} & {\color[HTML]{212121} 0.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{50\%}}} & {\color[HTML]{212121} -1.15e-01} & {\color[HTML]{212121} 6.16e-02} & {\color[HTML]{212121} 5.24e-02} & {\color[HTML]{212121} 1.31e-01} & {\color[HTML]{212121} 5.00e+01} & {\color[HTML]{212121} 0.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{75\%}}} & {\color[HTML]{212121} 3.79e+00} & {\color[HTML]{212121} 2.49e+00} & {\color[HTML]{212121} 2.29e+00} & {\color[HTML]{212121} 2.52e+00} & {\color[HTML]{212121} 7.00e+01} & {\color[HTML]{212121} 0.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{max}}} & {\color[HTML]{212121} 5.67e+01} & {\color[HTML]{212121} 3.09e+01} & {\color[HTML]{212121} 2.51e+01} & {\color[HTML]{212121} 3.73e+01} & {\color[HTML]{212121} 9.00e+01} & {\color[HTML]{212121} 0.00e+00}
\end{tabular}
\caption{Healthy gearbox data statistics}
\label{t1}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}r 
>{\columncolor[HTML]{FFFFFF}}r 
>{\columncolor[HTML]{FFFFFF}}r 
>{\columncolor[HTML]{FFFFFF}}r 
>{\columncolor[HTML]{FFFFFF}}r 
>{\columncolor[HTML]{FFFFFF}}r }
\multicolumn{1}{r}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{}}} & {\color[HTML]{212121} \textbf{Sensor 1}} & {\color[HTML]{212121} \textbf{Sensor 2}} & {\color[HTML]{212121} \textbf{Sensor 3}} & {\color[HTML]{212121} \textbf{Sensor 4}} & {\color[HTML]{212121} \textbf{load}} & {\color[HTML]{212121} \textbf{failure}} \\ \cline{2-7} 
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{count}}} & {\color[HTML]{212121} 1.01e+06} & {\color[HTML]{212121} 1.01e+06} & {\color[HTML]{212121} 1.01e+06} & {\color[HTML]{212121} 1.01e+06} & {\color[HTML]{212121} 1.01e+06} & {\color[HTML]{212121} 1.01e+06} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{mean}}} & {\color[HTML]{212121} -1.04e-03} & {\color[HTML]{212121} 1.73e-03} & {\color[HTML]{212121} 7.31e-04} & {\color[HTML]{212121} 1.34e-03} & {\color[HTML]{212121} 4.55e+01} & {\color[HTML]{212121} 1.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{std}}} & {\color[HTML]{212121} 4.60e+00} & {\color[HTML]{212121} 4.39e+00} & {\color[HTML]{212121} 3.81e+00} & {\color[HTML]{212121} 4.41e+00} & {\color[HTML]{212121} 2.90e+01} & {\color[HTML]{212121} 0.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{min}}} & {\color[HTML]{212121} -2.53e+01} & {\color[HTML]{212121} -3.25e+01} & {\color[HTML]{212121} -2.59e+01} & {\color[HTML]{212121} -2.74e+01} & {\color[HTML]{212121} 0.00e+00} & {\color[HTML]{212121} 1.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{25\%}}} & {\color[HTML]{212121} -2.77e+00} & {\color[HTML]{212121} -2.47e+00} & {\color[HTML]{212121} -2.03e+00} & {\color[HTML]{212121} -2.37e+00} & {\color[HTML]{212121} 2.00e+01} & {\color[HTML]{212121} 1.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{50\%}}} & {\color[HTML]{212121} -5.43e-02} & {\color[HTML]{212121} 1.26e-01} & {\color[HTML]{212121} 4.50e-02} & {\color[HTML]{212121} 1.01e-01} & {\color[HTML]{212121} 5.00e+01} & {\color[HTML]{212121} 1.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{75\%}}} & {\color[HTML]{212121} 2.67e+00} & {\color[HTML]{212121} 2.68e+00} & {\color[HTML]{212121} 2.10e+00} & {\color[HTML]{212121} 2.44e+00} & {\color[HTML]{212121} 7.00e+01} & {\color[HTML]{212121} 1.00e+00} \\
\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{max}}} & {\color[HTML]{212121} 2.64e+01} & {\color[HTML]{212121} 2.47e+01} & {\color[HTML]{212121} 2.69e+01} & {\color[HTML]{212121} 3.24e+01} & {\color[HTML]{212121} 9.00e+01} & {\color[HTML]{212121} 1.00e+00}
\end{tabular}
\caption{Broken tooth gearbox data statistics}
\label{t2}
\end{table}

\subsection*{Data inspection}

We can plot the data for healthy and broken gearbox to get a better understanding. 

\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.80\columnwidth]{s1}}
\caption{Sensor 1 data in different time-windows for healthy and broken gear}
\end{figure}

\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.80\columnwidth]{s2}}
\caption{Sensor 2 data in different time-windows for healthy and broken gear}
\end{figure}

\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.80\columnwidth]{s3}}
\caption{Sensor 3 data in different time-windows for healthy and broken gear}
\end{figure}

\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.80\columnwidth]{s4}}
\caption{Sensor 4 data in different time-windows for healthy and broken gear}
\end{figure}

\subsection*{Data preparation}

Using the code below, we can split the dataset into train-test. Note that the split ratio is 80\%-20\%. 

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
from sklearn.model_selection import train_test_split

# The last element contains the labels
labels = healthy_df['failure'].values

# The other data points are the electrocadriogram data
data = healthy_df[['a1', 'a2', 'a3', 'a4']].values

X_train, X_test, y_train, y_test = train_test_split(
    data, labels, test_size=0.2, random_state=21
)
\end{lstlisting}
\end{minipage}}\end{center}

We have also normalized the dataset to be between 0-1 using the formula below. 

\begin{equation}
X = \frac{X-X_{min}}{X_{max}-X_{min}}
\end{equation}

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
min_val = tf.reduce_min(X_train)
max_val = tf.reduce_max(X_train)

X_train = (X_train - min_val) / (max_val - min_val)
X_test = (X_test - min_val) / (max_val - min_val)

X_train = np.array(X_train)
X_test = np.array(X_test)
\end{lstlisting}
\end{minipage}}\end{center}

The dataset is a time series and we can use RNN or LSTM networks. For this matter, we take 200 consecutive data points and concatenate them to become one single data. So now we have a total of 4063 train data (time series data) and 1015 test data. 

\subsection*{Neural network models}


When training the network, we would only use the correct (healthy) data. This is because in anomaly detection, the auto-encoder network learns to reconstruct a certain time series signal. 

\subsubsection*{LSTM autoencoder model} The autoencoder network first reduces the input size of (200,4) to (200, 16) and then (4). After we repeat this vector to obtain a size of (200,4). This then becomes (200, 16) and finally (200,4) which is the input dimension. 

\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.9\columnwidth]{LSTM}}
\caption{LSTM-autoencoder neural network architecture}
\end{figure}

The model code is shown below. 

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
inputs = Input(shape=(200, 4))
L1 = LSTM(16, activation='relu', return_sequences=True, 
            kernel_regularizer=regularizers.l2(0.00))(inputs)
L2 = LSTM(4, activation='relu', return_sequences=False)(L1)
L3 = RepeatVector(200)(L2)
L4 = LSTM(4, activation='relu', return_sequences=True)(L3)
L5 = LSTM(16, activation='relu', return_sequences=True)(L4)
output = TimeDistributed(Dense(4))(L5)    
model = Model(inputs=inputs, outputs=output)

model.compile(optimizer='adam', loss='mae')
model.summary()

nb_epochs = 150
batch_size = 1024
history = model.fit(data_train, data_train, epochs=nb_epochs, batch_size=batch_size, validation_split=0.05).history
\end{lstlisting}
\end{minipage}}\end{center}

\subsubsection*{CNN autoencoder model}

Another network that can be a suitable candidate for the problem is a 1D CNN-autoencoder network. 1D CNN can be used to analyze time series data. 

The CNN-autoencoder network has 3 1D-CNN layers to reduce the data dimension ($200\rightarrow128\rightarrow64\rightarrow32$), and 3 1D-CNN transpose layers to decode ($32\rightarrow64\rightarrow128\rightarrow200$). 

\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.9\columnwidth]{CNN}}
\caption{CNN-autoencoder neural network architecture}
\end{figure}

The model code is shown below. 

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
inputs = Input(shape=(200, 4))
x = Conv1D(128,3, activation='relu', dilation_rate=2)(inputs)
x2 = Conv1D(64,3, activation='relu', dilation_rate=2)(x)
x3 = Conv1D(32,3, activation='relu', dilation_rate=2)(x2)

d3 = Conv1DTranspose(32 ,3,activation='relu', dilation_rate=2)(x3)
d4 = Conv1DTranspose(64,3,activation='relu', dilation_rate=2)(d3)
d5 = Conv1DTranspose(128,3,activation='relu', dilation_rate=2)(d4)

decoded = Conv1D(4,1,strides=1, activation='sigmoid')(d5)
model= Model(inputs, decoded)
model.compile(optimizer='adam', loss='mae')
model.summary()

nb_epochs = 150
batch_size = 1024
history = model.fit(data_train, data_train, epochs=nb_epochs, batch_size=batch_size, validation_split=0.05).history
\end{lstlisting}
\end{minipage}}\end{center}

Adam optimizer usually has good results in training all kinds of neural networks, so we have chosen that here. The loss function is also mean absolute error (MAE) since we have a regression problem.
The batch size and number of epochs were selected to be 1024 and 150 for both CNN and LSTM networks.

\subsection*{Train Results}

We can plot the loss vs epochs in both networks. 

\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.8\columnwidth]{LSTMa}}
\caption{LSTM-autoencoder loss vs epochs}
\label{fig7}
\end{figure}

\begin{figure}[H]
\centering
\subfloat{\includegraphics[width=0.8\columnwidth]{CNNa}}
\caption{CNN-autoencoder loss vs epochs}
\label{fig8}
\end{figure}

As it can be seen in \ref{fig7} and \ref{fig8} the loss decrease smoother in LSTM compared to the CNN network but CNN can achieve lower loss overall (CNN val-loss is 0.0067 compared to LSTM val-loss of 0.038).

\subsection*{Histogram of healthy and broken tooth gearbox data}

Histogram diagrams of LSTM and CNN networks for both correct and anomaly data is shown in \ref{fig9} and \ref{fig10} respectively.

\begin{figure}[H]
\centering
\subfloat[Healthy gearbox]{\includegraphics[width=0.45\columnwidth]{LSTMc.png}}\quad
\subfloat[Anomaly]{\includegraphics[width=0.45\columnwidth]{LSTMan.png}}
\caption{histogram diagram for loss function of LSTM network}
\label{fig9}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Healthy gearbox]{\includegraphics[width=0.45\columnwidth]{CNNc.png}}\quad
\subfloat[Anomaly]{\includegraphics[width=0.45\columnwidth]{CNNan.png}}
\caption{histogram diagram for loss function of CNN network}
\label{fig10}
\end{figure}

\subsection*{Results analysis}

These observations can be made from the histograms

\begin{enumerate}
	\item The CNN network has a smaller final loss compared to LSTM 
	\item Number of correct examples were more than anomaly examples
	\item both networks have less total loss for correct examples than anomaly examples
\end{enumerate}

So both networks trained how to reconstruct the correct examples (data or signals) with lower error(loss). Since they cannot reconstruct the anomaly examples, they will have a higher output error. We must choose a threshold for error 
To summarize, when we give an input to the network, the network reconstructs it and we can calculate the reconstruction error. We need to define a threshold to classify examples with less error than that to be correct and errors higher than that to be anomaly. 

To calculate this threshold, we used formula shown.

\begin{equation}
Threshold = MEAN(losses)+STD(losses)
\end{equation}

The evaluation results of networks on the test data are listed in \ref{t3},

\begin{table}[H]
\centering
\begin{tabular}{
>{\columncolor[HTML]{FFFFFF}}l 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c }
{\color[HTML]{212121} \textbf{}} & {\color[HTML]{212121} \textbf{CNN NETWORK}} & {\color[HTML]{212121} \textbf{LSTM NETWORK}} \\ \cline{2-3} 
\multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{accuracy}}} & {\color[HTML]{212121} 92.5\%} & {\color[HTML]{212121} 81.4\%} \\
\multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{precision}}} & {\color[HTML]{212121} 100\%} & {\color[HTML]{212121} 79\%} \\
\multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{Recall}}} & {\color[HTML]{212121} 85\%} & {\color[HTML]{212121} 85.5\%} \\
\multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{212121} \textbf{F1}}} & {\color[HTML]{212121} 92\%} & {\color[HTML]{212121} 82\%}
\end{tabular}
\caption{Evaluation of networks}
\label{t3}
\end{table}

As it can be seen in \ref{t3}, the CNN network has better performance compared to LSTM. Speed of training for LSTM networks on average was $1 \frac{s}{step}$ and for CNN network was $96 \frac{ms}{step}$.

\subsection*{Sending Email}

After training the model, we can use it to predict any signal data from a working gearbox. After prediction, if the reconstruction error was higher than the threshold calculated earlier, we can send an email to the clients indicating that there is a problem with the gearbox. 
For demonstration we used the email address arduinokntu@gmail.com as server and najafi@kntu.ac.ir and j.khorramdel96@gmail.com as clients.

The code for this part is shown below.

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
import smtplib

reconstructions = model.predict(data_train[0:1,:,:])
loss = tf.keras.losses.mae(reconstructions, data_train[0:1,:,:])
loss=np.mean(loss)

if (loss>threshold):
   server=smtplib.SMTP_SSL("smtp.gmail.com",465)
   server.login("arduinokntu@gmail.com","xxxxxxxxx")
   server.sendmail("arduinokntu@gmail.com", 
   ["najafi@kntu.ac.ir","j.khorramdel96@gmail.com"], 
   "this is an email from [hoseinzadeh,borhani,ghanbarzadeh] IOT group.")
\end{lstlisting}
\end{minipage}}\end{center}

\subsection*{Conclusion}

In this report we trained a CNN and a LSTM network for anomaly detection. CNN can achieve better results with higher speed of training in comparison with LSTM. At the end if network recognize an anomaly signal, an email is sent from server to clients.

\newpage
\section*{Part II}
In this section, an attendee system is implemented. The goal is designing an appropriate neural network for face recognition using federated learning. In the first step, two main architectures have been studied. Then one of them is applied for the federated learning method.

\subsection*{Face Recognition Architecture}
In this part, at first the Siamese Networks and Multi-task Cascaded Convolutional Networks (MTCNN) are introduced briefly. Then the first structure is studied and improved. At the end of this section, the second method is discussed.

\subsubsection*{About Siamese Networks}
A Siamese Network is a type of network architecture that contains two or more identical subnetworks used to generate feature vectors for each input and compare them. Siamese Networks can be applied to different use cases, like detecting duplicates, finding anomalies, and face recognition.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{Figures/siamese.jpeg}
	\caption{Siamese Network Structure}
	\label{fig:f}
\end{figure}

In other words, a Siamese Network consists of twin networks which accept distinct inputs but are joined by an energy function at the top. This function computes a metric between the highest level feature representation on each side. The parameters between the twin networks are tied. Weight tying guarantees that two extremely similar images are not mapped by each network to very different locations in feature space because each network computes the same function. The network is symmetric, so that whenever we present two distinct images to the twin networks, the top conjoining layer will compute the same metric as if we were to we present the same two images but to the opposite twins. Intuitively instead of trying to classify inputs, a siamese network learns to differentiate between inputs, learning their similarity. The loss function used is usually a form of contrastive loss.

\subsubsection*{MTCNN: Face Detection Method}
Multi-task Cascaded Convolutional Networks (MTCNN) is a framework developed as a solution for both face detection and face alignment. The process consists of three stages of convolutional networks that are able to recognize faces and landmark location such as eyes, nose, and mouth.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{Figures/mtcnn2.png}
	\caption{MTCNN Network}
	\label{fig:mt}
\end{figure}


As is shown in \ref{fig:mt_s}, the first stage is a fully convolutional network (FCN). The difference between a CNN and a FCN is that a fully convolutional network does not use a dense layer as part of the architechture. This Proposal Network is used to obtain candidate windows and their bounding box regression vectors.
Bounding box regression is a popular technique to predict the localization of boxes when the goal is detecting an object of some pre-defined class, in this case faces. After obtaining the bounding box vectors, some refinement is done to combine overlapping regions. The final output of this stage is all candidate windows after refinement to downsize the volume of candidates.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.52]{Figures/mtcnn_structure.jpg}
	\caption{MTCNN Structure}
	\label{fig:mt_s}
\end{figure}

The second step is The Refine Network(R-Net). All candidates from the P-Net are fed into the Refine Network. Notice that this network is a CNN, not a FCN like the one before since there is a dense layer at the last stage of the network architecture. The R-Net further reduces the number of candidates, performs calibration with bounding box regression and employs non-maximum suppression (NMS) to merge overlapping candidates.
The R-Net outputs whether the input is a face or not, a 4 element vector which is the bounding box for the face, and a 10 element vector for facial landmark localization.

The last stage is The Output Network (O-Net). This stage is similar to the R-Net, but this Output Network aims to describe the face in more detail and output the five facial landmarks’ positions for eyes, nose and mouth.

In this project, the MTCNN method is used in order to detect faces in pictures.
\subsubsection*{First Architecture}
In this part, the first network architecture is studied. Since the project is face recognition, the \emph{"fetch lfw people"} dataset is a suitable choice for training the network. The main structure of the network is shown in \ref{fig:f3}. This part remains constant during our study. In other words, the other part is changed to improve the algorithm's functionality. 

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.3]{siamese.png}
	\caption{Siamese Network Layers}
	\label{fig:f3}
\end{figure}

The \ref{fig:f3} represents the first design of the Siamese Network. As mentioned before, Siamese Network is used to find similarities between two images, while the Lambda layer is applied to calculate the euclidean distance of extracted features. 

%\begin{figure}[!h]
%	\centering
%	\includegraphics[scale=0.5]{Figures/s1.png}
%	\caption{Siamese Network (model 1)}
%	\label{fig:f2}
%\end{figure}

The first idea is to use the VGG19 structure for transfer learning. For having better performance, 10 percent of layers are frozen, and the remaining ones become trainable. Then Global average pooling and dense layers are implemented, respectively. As it is shown in \ref{table:t1}, this structure has good performance on the training set, but the validation accuracy is not convincing. The reason is that overfitting has occurred.

For solving the previous problem, the VGG19 architecture is removed. Instead of that, convolutional and max-pooling layers are implemented. The structure of this development is shown in \ref{fig:f3}.

%\begin{figure}[!h]
%	\centering
%	\includegraphics[scale=0.3]{siamese.png}
%	\caption{Siamese Network Architecture}
%	\label{fig:f3}
%\end{figure}

The other method which is used in this model is using dropout layer. It can help the network to have a better result. As it is shown in \ref{table:t1}, this model has a considerable improvement compared to model 1. The test accuracy reaches almost 83 percent. 
It should be mentioned that the Adam optimizer is applied for both model 1 and model 2. Also, the learning rate has an initial value of 0.01, and no scheduler is used.

In order to check the activation function effect, instead of using ReLU, Leaky ReLU is applied (model 3). The validation accuracy reaches 87 percent. In the next step, the effect of choosing an optimizer is examined. The Adam optimizer is applied for the previous models. In model 4, the SGD with momentum is used instead of the Adam optimizer. The momentum equals 0.9. The result is shown in \ref{table:t1}.

One of the effective methods is changing the learning rate schedulers. The constant learning rate is the default learning rate schedule in the SGD optimizer in Keras. Momentum and decay rate are both set to zero by default. It is tricky to choose the right learning rate. Two different learning rate schedulers are examined in this part: Exponential Decay and Cosine Decay. These two are implemented to the models with Adam and SGD optimizer. Since the Adam optimizer has not good performance with schedulers, the SGD is used instead. Model 5 benefits the Cosine decay as it performs better than the Exponential Decay. 

Some references mention that when convolutional layers are implemented, the Drop Block method may have a better result than the Dropout layer. When the dropout layer applies, the image features are removed randomly according to the dropout rate. Although in the drop block method, some regions on feature map are removed. If the dimension of the block is set to 5 by 5, then this size of the region will be eliminated. In model 6, the drop block layers are used such that the probability of preserving an m-by-m region on the feature map was set to 90 percent.

\begin{table}[h]
	\centering
	\begin{tabular}{ |c||c|c|  }
		\hline
		%\multicolumn{3}{|c|}{Different Model} \\
		%\hline
		Model Number& Best Train Accuracy &Best Validation Accuracy\\
		\hline
		model 1 & 99.9 & 63.9 \\
		model 2 & 97.8 & 82.9 \\
		model 3 & 99.6 & 87.1 \\
		model 4 & 97.1 & 86.2 \\
		model 5 & 99.9 & 88.1 \\
		model 6 & 100 & 88.9 \\
		\hline
	\end{tabular}
	\caption{Accuracy Results of Each Model}
	\label{table:t1}
\end{table}

The loss is plotted for the FaceNet network which is trained using our dataset. 

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.4]{2loss.png}
	\caption{FaceNet model loss}
	\label{fig:f2}
\end{figure}

It is noteworthy that the other methods are applied like augmentation, using different initial learning rates, changing transfer learning networks like VGG19 and VGG16, etc. As these ideas do not perform well, the results are not shown.

\subsubsection*{Second Architecture}
In this section, the FaceNet structure is implemented. FaceNet is a face recognition system developed in 2015 by researchers at Google that achieved then state-of-the-art results on a range of face recognition benchmark datasets. The FaceNet system can be used thanks to multiple third-party open-source implementations of the model and the availability of pre-trained models broadly. The FaceNet system can be used to extract high-quality features from faces, called face embeddings, that can then be used to train a face identification system. The model is a special case of the otherwise generalized inception model that has been tuned for face recognition task. It as a deep convolution network the accepts pixels of an image as an input and outputs a 128 dimension vector that is known as “face embedding”. To learn the weights, the model optimizes a triplet loss function wherein the images are mapped to an Euclidean space and distances between the mappings or embeddings are calculated using the K-means clustering algorithm. The embeddings the fall in the same cluster are said to be of the same person. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{Figures/FaceNet.png}
	\caption{The Triplet Loss minimizes the distance between an anchor and a positive, both of which have the same identity, and
		maximizes the distance between the anchor and a negative of a
		different identity.}
	\label{fig:f4}
\end{figure}

To put simply, if the distances between any two embedding are lesser than some threshold value then those embeddings belong to the same person. The model not only learns to reduce the distances between homogeneous faces but also to maximize the distances between heterogeneous faces. The anchor represents a control image and the positive represents an image with the same person as in the anchor image and the negative represents that of a different person. In the training phase, the model learns to form clusters having embeddings of the same person. After completion of training, the model can map an image into a cluster containing the mappings of other images of the same person thereby, discovering the identity of the person.

The following steps were took during the project:

\textbf{Step One:} In the first step, the FaceNet model is imported. This model is needed for extracting image features. In this part, faces from images are recognized and then turned into embeddings. 

\textbf{Step Two:} One of the essential stages is to choose or collect an appropriate dataset. In this study, two datasets are used. One of them is "5-celebrity-faces-dataset". The other one is collected by our team members which contains Armin Ghanbarzadeh, Mohammad Hosseizadeh, Reza Behbahani and Navid Faraji photos.

\textbf{Step Three:} In the next step, the data should be preprocessed. In this project, all images are resized to 160 by 160, and they are RGB. Then the dataset is split to train and test sets. Following previous steps, the embeddings are extracted after images are ready. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.9]{facenet}
	\caption{Model Structure}
	\label{fig:f5}
\end{figure}

\textbf{Step Four:} To complete data preprocessing, the embeddings should first be pairs and the labels prepared according to those pairs. By using Permutation rules, the embeddings get pairs. Then if two embeddings belong to the same person, the label sets to 1. Otherwise, it is 0.

\textbf{Step Five:} Now the embeddings are extracted and labels are ready. Since the dataset has new people inside, a few layers are implemented. We choose the MLP structure to train the model for recognizing the images in the new datasets. As it is shown in \ref{fig:f5}, two Dense layers are used. Two embeddings enter the Lambda layer first. This layer is used for calculating the Euclidean distance between two inputs. Then the hidden layer with 100 neurons is applied, and then it is the output layer.


\newpage
\subsection*{Implementation}

The Code base has 3 important files regarding the neural network. First they will be discussed. After, we can demonstrate the Federated learning mechanism implemented. 

\subsubsection*{face\_recognition.py:} This file contains the main class for our neural network. We can easily use this to train our model or test it when needed. It created the model architecture when an object of the class is made. After, the user can specify a weights file to load and perform a prediction using \textit{predict()} on a dataset. 

We can also pass a train dataset to the \textit{fit()} function to train the model in place. Some helper functions are also written to easily load and save weights from files which will come in handy later. 

The file is shown below (Some basic functions parts were omitted)

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
class face_recognition_network:

    def __init__(self):
        self.model = self.make_model()
        self.weights_version = -1
        self.WEIGHTS_PATH = 'models'

    def make_model(self):
        input_1 = tf.keras.layers.Input(shape=(128,))
        input_2 = tf.keras.layers.Input(shape=(128,))
        distance = tf.keras.layers.Lambda(euclidean_distance)
        			([input_1, input_2])
        x = Dense(100, activation='relu')(distance)
        output = Dense(1, activation='sigmoid')(x)
        model = tf.keras.Model(inputs=[input_1, input_2], 
        outputs=output)
        return model

    def fit(self, x_train, x_val, y_train, y_val, epochs):
        self.model.compile(optimizer='adam', 
        					loss='binary_crossentropy', 
        					metrics=["accuracy"])
        history = self.model.fit([x_train[:,0], x_train[:,1]], 
        							y_train[:], epochs=epochs, 
                                    validation_data=[[x_val[:,0], 
                                    x_val[:,1]], y_val[:]])
        self.weights_version += 1
        return history.history

    def predict(self, x_test):
        x_test = [x_test[:, 0],x_test[:, 1]]
        y_test = self.model.predict(x_test)
        # y_test = np.where(y_test, y_test>0.7, 1)
        return y_test
    
    def load_weights(self, weights=None):
        ...
    
    def save_weights(self, weights_file=None):
        ...
\end{lstlisting}
\end{minipage}}\end{center}

\subsubsection*{test\_with\_images.py:} This script uses the trained neural network with the latest weights to predict on test images placed in a folder. It uses MTCNN to detect faces and the bounding boxes within the test images. It then calculates the face embeddings using Facenet and compares the calculated embeddings to the images in our database. Based on the results, it predicts who the face belonged to. If it cannot find anyone in the database with reasonable similarity, it outputs unknown. The function \textit{test\_model\_with\_image} will be shown later. 

The files code is shown below.

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
d = update_known_embeddings('update')
nn = face_recognition_network()
nn.load_weights('latest')

test_images = glob.glob('test_images/*.jpg')
OUTPUT_FOLDER= 'test_results'

for image_path in test_images:
    image = Image.open(image_path)
    image = image.convert('RGB')
    image_array = np.asarray(image)
    r = test_model_with_image(image_array, nn, d)
    for face in r:
        x1, y1, x2, y2 = face['x1'], face['y1'], 
        					face['x2'], face['y2']
        image_array = cv2.rectangle(image_array, (x1, y1), 
        				(x2, y2), (36,255,12), 1)
        image_array = cv2.putText(image_array, face['name'], 
        				(x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 
        				0.9, (36,255,12), 2)
    image_name = os.path.split(image_path)[-1]
    Image.fromarray(image_array).save(os.path.join(OUTPUT_FOLDER, image_name))
\end{lstlisting}
\end{minipage}}\end{center}

\subsubsection*{train\_head\_model.py:} This script contains a function to train the neural network on. Since we only train the network when the server tells us, this function is imported in the \textit{client.py} code shown later.

\textit{make\_train\_dataset} is used to make the training set with the known embeddings from the images. After that we can simply pass the prepared dataset to the models fit function. 

The code is shown below.

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
def train_model(epochs=20, weights='latest'):
    database = update_known_embeddings('update')
    X_train, X_val, y_train, y_val = make_train_dataset(database)
    nn = face_recognition_network()
    nn.load_weights(weights)
    h = nn.fit(X_train, X_val, y_train, y_val, epochs)
    nn.save_weights()
    return nn
\end{lstlisting}
\end{minipage}}\end{center}

\subsubsection*{utils.py:} This file has some functions that we used in the main scripts. We will discuss them here. 

\textit{extract\_faces}: This function uses MTCNN to detect faces in a given image. It is used when preparing the training dataset and when we want to test a image. 

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
def extract_faces(image, required_size=(160, 160)):
    detector = MTCNN()
    results = detector.detect_faces(image)
    faces = []
    for i in range(len(results)):
        x1, y1, w, h = results[i]['box']
        x1, y1 = np.abs(x1), np.abs(y1)
        x2, y2 = x1 + w, y1 + h
        face = image[y1:y2, x1:x2]
        faces.append([face, (x1,y1), (x2,y2)])
    return faces
\end{lstlisting}
\end{minipage}}\end{center}

\textit{get\_embedding}: This function runs the Facenet model on images and outputs the normalized embeddings for those images. 

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
def get_embedding(faces_array):
    face_net = load_model("models/facenet")
    mean, std = faces_array.mean(), faces_array.std()
    samples = (faces_array - mean) / std
    embeddings = face_net.predict(samples)
    in_encoder = Normalizer(norm='l2')
    embeddings = in_encoder.transform(embeddings)
    return embeddings
\end{lstlisting}
\end{minipage}}\end{center}

\textit{update\_known\_embeddings}: Whether we want to train or test our network, the images stored from known people are not actually necessary. We only need the image embeddings to train or compare an unknown image with. This script makes the database file which has the name of each image file with its class (human name) and a 128 vector embedding. Every time it is run, it only checks for changes in the image folders and updates this database file. Having this file on hand proves to be very usefull, especially on weaker hardware like a raspberry PI. 

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
def update_known_embeddings(embedding_file=None):
	... 
    return database
\end{lstlisting}
\end{minipage}}\end{center}

\textit{make\_train\_dataset}: This function makes a dataset given a set of known embeddings. As discussed previously, it combines two images together and assigns a label. If the images are from the same person, the label is one otherwise 0. It can also be noted that a 85-15 percent train-test split is used. This is because we don't have many data (around 15 images per person) so we must use as much as possible for training.

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
def make_train_dataset(database):
    ...
    trainX, testX, trainy, testy = train_test_split(X, y, train_size=0.85)
    ...
    return X_train, X_val, y_train, y_val
\end{lstlisting}
\end{minipage}}\end{center}

\textit{test\_model\_with\_image}: This function makes predictions for a test image. It sums the model confidence on all know people and predicts a name for each face in the image. It also returns the bounding box so we can draw a box around the faces. 

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
def test_model_with_image(image_array, model, database):
    faces_data = extract_faces(image_array)
	...
        res.append({'name':n,
                    'x1':faces_data[i][1][0],
                    'y1':faces_data[i][1][1],
                    'x2':faces_data[i][2][0],
                    'y2':faces_data[i][2][1]})
    return res
\end{lstlisting}
\end{minipage}}\end{center}

\textit{average\_weights}: This function is used in the federated learning part. It takes a set of model weights and averages them. The server uses this function to average the weights received from the clients so it can make a new weight and send it back to them.

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
def average_weights(weights_paths, save_path):
    ...
    return new_weights
\end{lstlisting}
\end{minipage}}\end{center}

\subsection*{Client-Server interaction and Federated learning}

We can test the networks before training on a few images. The results are shown in \ref{f10}

\begin{figure}[H]
	\centering
	\includegraphics[scale=1.5]{before.png}
	\caption{Test results without any training}
	\label{f10}
\end{figure}

As it can be seen, it cannot correctly recognize any face in the images. It even misclassified some faces to some classes in the database. 

Now we can run the server-client scripts. For the environment demonstrated here we have 2 clients. 

Client 1 has images of:
\begin{itemize}
	\setlength\itemsep{0.01em}
	\item Armin Ghanbarzadeh
	\item Mohammad Hoseinzadeh
	\item Ben Afflek
	\item Elton John
\end{itemize}

Client 2 has images of:
\begin{itemize}
	\setlength\itemsep{0.01em}
	\item Navid Faraji
	\item Madonna
	\item Jerry Seinfeld
	\item Mindy Kaling
\end{itemize}

\textbf{Step One:} Server connects to clienct and sends current weights to them.

\textit{Server-side Code:}

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
IP = "127.0.0.1"
PORT = 1234
NUMBER_OF_CLIENTS = 2

s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
s.bind((IP, PORT))
s.listen(5)

clients = []
for i in range(NUMBER_OF_CLIENTS):
    clientsocket, address = s.accept()
    filename = os.path.join(WEIGHTS_FOLDER, weight_file)
    with open(filename, 'rb') as file:
        sendfile = file.read()
    sendfile = bytes(f"{len(sendfile):<{HEADERSIZE}}",'utf-8')+sendfile
    clientsocket.send(sendfile)
\end{lstlisting}
\end{minipage}}\end{center}

\textit{Client-side Code:}

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
IP = "127.0.0.1"
PORT = 1234
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.connect((IP, PORT))
with open(os.path.join(WEIGHTS_FOLDER, "weights_{:04d}.h5".format(version)),'wb') as file:
    message_header = s.recv(HEADERSIZE)
    message_length = int(message_header.decode('utf-8').strip())
    weights = s.recv(message_length)
    file.write(weights)
\end{lstlisting}
\end{minipage}}\end{center}

\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{r1}
	\caption{Step One (left: server, right:clients)}
	\label{f10}
\end{figure}

\textbf{Step Two:} Now the clients start to train with the new weights. They first update the database based on any images they have, then create a training dataset. 

\textit{Client-side Code:}

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
train_model(weights="weights_{:04d}.h5".format(version))
\end{lstlisting}
\end{minipage}}\end{center}

\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{r2}
	\caption{Step Two (left: client 1, right:client 2)}
	\label{f10}
\end{figure}


\textbf{Step Three:} The clients send the weights after training back to the server. 

\textit{Server-side Code:}

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
for i in range(NUMBER_OF_CLIENTS):
    clientsocket, address = clients[i]
    with open(new_weights_filename,'wb') as file:
        message_header = clientsocket.recv(HEADERSIZE)
        message_length = int(message_header.decode('utf-8').strip())
        weights = clientsocket.recv(message_length)
        file.write(weights)
\end{lstlisting}
\end{minipage}}\end{center}

\textit{Client-side Code:}

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
with open(filename, 'rb') as file:
    sendfile = file.read()
sendfile = bytes(f"{len(sendfile):<{HEADERSIZE}}",'utf-8')+sendfile
s.send(sendfile)
\end{lstlisting}
\end{minipage}}\end{center}

\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{r3}
	\caption{Step Three (left: server, right:clients)}
	\label{f10}
\end{figure}

\textbf{Step Four:} The Server gets the weights, averages them and sends them back to the clients. 

\textit{Server-side Code:}

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
new_weights = average_weights(client_weights, save_path)
for file in client_weights:
    os.remove(file)
for i in range(NUMBER_OF_CLIENTS):
    clientsocket, address = clients[i]
    with open(weights_file, 'rb') as file:
        sendfile = file.read()
    sendfile = bytes(f"{len(sendfile):<{HEADERSIZE}}",'utf-8')+sendfile
    clientsocket.send(sendfile)
\end{lstlisting}
\end{minipage}}\end{center}

\textit{Client-side Code:}

\begin{center}{\begin{minipage}{0.9\linewidth}
\begin{lstlisting}[language=Python, basicstyle=\fontsize{10}{10}\selectfont\ttfamily]
with open(os.path.join(WEIGHTS_FOLDER, "weights_{:04d}.h5".format(version+1)),'wb') as file:
    message_header = s.recv(HEADERSIZE)
    message_length = int(message_header.decode('utf-8').strip())
    weights = s.recv(message_length)
    file.write(weights)
\end{lstlisting}
\end{minipage}}\end{center}

\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{r4}
	\caption{Step Four (left: server, right:clients)}
	\label{f10}
\end{figure}

\textbf{Step Five:} We test the clients again with the new weights. The results are shown in the images below. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=1.2]{after.png}
	\caption{Test results after training}
	\label{f10}
\end{figure}

The unknown labels were of people not in the database, so they are labeled correctly aswell. 
%----------------------------------------------------------------------------------------
%------------------------------------------------

\end{document}
